CS 341
===

### Lecture 1: Sep 7, 2017
**Outline**: How to find the best algorithms for problems.

1. How to design algorithms
    - General paradigms
        - divide and conquer
        - greedy
        - dynamic programming
        - reductions
    - Basic repertoire
        - sorting
        - string algorithms
        - graph algorithms
        - linear programming
        - numerical algorithms
2. How to analyze algorithms
    - how good is your algorithm
    - Time, space
    - O notation, worst case / average case
    - models of computation
3. Lower Bounds: How do we have the best algorithm
    - models of computations
    - basic lower bounds
    - NP-completeness, undecidability

**Case Study**: Convex Hull
Given a set $X$ of $n$ points, find the minimum convex set that contains $X$. 
This is equivalent to finding the set of lines $L$ through two or more points with all other points to one side. 

*Alg 1*: Try all $n \choose 2$ pairs of points. Construct lines. Test all other $n-2$ points.  
This will take $O(n^3)$ time. 

*Alg 2*: Find the lines in cyclic orders
Given line $l$ through $s$, find the next line $l'$ through point s, which takes $O(n)$ time. 
Total time is $O(n^2)$. 
Let $h$ be the size of convex hull, then time is $O(nh)$. 

*Alg 3*: Use sorting
Sort the points by x-coordinates. 
There are two chains between the min x-coord and the max x-coord, called the upper convex hull and the lower convex hull. 
We can find the upper and lower convex hull in $O(n)$ time after sorting. 
Takes $O(n \log n)$ time. 

*Alg 4*: Divide and conquer
Divide the points in half by x-coordinates. 
Recurse on each side. 
Find the upper and lower bridge (takes $O(n)$ time) to join the two convex hulls. 
$T(n) = 2T(n/2) + O(n)$
This is similar to merge sort. Takes $O(n \log n)$ time. 

If the expected $h$ is small, then $O(nh)$ is more preferable. 
The ultimate convex hull algorithm takes $O(n \log h)$ time. 

*Lower bounds*: 
* must restrict the model of computation 
* require that convex hull is ordered polygon
Then convex hull has $\Omega (n \log n)$ lower bound, otherwise we can sort faster than $\Omega (n \log n)$. 

**Algorithms and Analysis**
An algorithm is a finite answer to an infinite question. 

Problem: Infinite set of inputs, specify corresponding output for each input. 
Algorithm: computational procedure that takes an input and computes the corresponding output. 
Analyzing algorithm: measure time and space as a function of input size. Time, space, input size measured by model of computation. 

### Lecture 2: Sep 12, 2017

####Model of computation
General purpose models can do anything a real computer can. 

*Model 1*: Pseudo-code, each line takes 1 time step
Initializing an array might be 1 line, but should cost $n$ time steps, where $n$ is the length of the array. 
With this caution, this model reflects reality as long as the problem fits in main memory and the numbers involved fits in one word. 

When dealing with large numbers, such as the Fibonacci sequence, the numbers grow so quickly that $2n$ steps does not reflect reality: $n=47$ causes overflow on 32 bit words. 

*Model 2*: Pseudo-code, take word size into account
Limit word size (constant or log n) or count cost of arithmetic on large words. 
E.g. a*b

\begin{aligned}
\text{size of number } a &= \text{# of bits in binary representation} \\
&= \lfloor \log a + 1 \rfloor $ \\
&= O(\log a)
\end{aligned}

a*b takes $O((\log a) (\log b)$. 

*Model 3*: RAM = Random Access Machine - abstracts assembly language
- random access means we can access memory location $i$
- each memory location holds one word and assume # bits in word $\geq \log n$, where $n = \text{input size}$ 

*Model 4*: Circuit family - abstracts hardware circuitry

*Model 5*: Turing machine - abstracts human computer working with pencil and paper. 
Note: The time to access memory location $i$ is proportional to $i$. 

Special purpose or "structured" models of computing:**
- comparison-based model for sorting. $\Omega(n \log n)$ lower bound
- arithmetic model

Our model of computing: 
- Pseudo-code and try to be realistic about what are elementary operations
- for graph algorithms, we'll assume $\log n$ bits per word and constant time for arithmetic on words. 

####Running Time of an Algorithm

Running-time depends on input
$T_A(I) = \text{ running time of algorithm } A \text{ on input } I$

We expect running time to increase as size of input increases. 
We express run time as a function of input size. 
The model of computing must say how to count input size. 
For given size $n$, there are a finite number of inputs. 
How do we combine running times to one number?

Worst case running time
$T_A(n) = \max \{T_A(I): I \text{ is an input of size } n \}$

Average case running time: replace "max" by "average"
Or use a more general probability distribution on inputs by assuming uniform distribution. 

When an algorithm uses random numbers, we use expected run-time
$T_A^E(I) = \text{ expected runtime on input } I$
We still use worst case over inputs: 
$T_A^E(n) = \max \{ T_A^E(I): I \text{ is an input of size } n\}$

*Challenge*: If you have a RAM model and allows unlimited number of bits for each memory locaiton and charge 1 for arithmetic and shifts, how can you sort in $O(n)$ steps? 
Hint: Do all $n^2$ pairwise comparisons in 1 subtraction. 


####Asymptotic Analysis of Algorithms

Recall running time of algorithms as function of input size $n$. 
$T(n) = \max\{T(I): I \text{ an input of size } n\}$
$T(I)$ and size($I$) measured according to model of computation. 

We want $T(n)$ to be
- simple to express; eg. $n^2$
- machine independent, thus ignore multiplicative factors (one machine might be twice as fast), thus ignore lower order terms ($n + 5 \leq 2n, n \geq 5$)

*Definition*: (Big Oh Notation)
Let $f(n)$, $g(n)$ be functions from $\mathbb{N}$ to $\mathbb{R} \geq 0$
$f(n)$ is $O(g(n))$ if there exists constants $c > 0$ and $n_0$ such that $f(n) \leq c \cdot g(n)$ for all $n > n_0$. 
We say that $f(n) \in O(g(n))$. 

Big oh gives an upper bound. Examples: 

- $T(n) = 5n^2 + 3n+25$ is $O(n^2)$
- $10^{100} \cdot n$ is $O(n)$
- $\log n$ is $O(n)$ but $n$ is not $O(\log n)$
- $2^{n+1}$ is $O(2^n)$
- $(n+1)!$ is not $O(n!)$

Properties of Big Oh

- max rule: $O(f(n)+g(n))$ is $O(\max\{f(n), g(n)\})$
- transitivity: $f(n) \in O(g(n))$ and $g(n) \in O(h(n))$ implies that $f(n) \in O(h(n))$

*Definitions*: 

- $f(n) \in \Omega(g(n))$ if there exists constants $c, n_0$ such that $f(n) \geq c \cdot g(n)$ for all $n \geq n_0$. 
- $f(n) \in \Theta(g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$. 
- $f(n)$ is $o(g(n))$ if for any constant $c > 0$, there exists a constant $n_0$ such that $f(n) \leq c \cdot g(n) \forall n \geq n_0$
	Equivalently, $\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$

Suppose the worst case run time of algorithm $A$ is $O(n^2)$, algorithm $B$ is $O(n \log n)$. 
We cannot determine which algorithm is better. 
To compare algorithms, we need tighter bounds. 

Typical run-times

$\log n$ - binary search
$n \log n$ - sorting
$n$ - find max
$n^2$ - insertion sort
$n^3$ - multiplying two n*n matrices
$n!$ - try all orderings of aset
$2^n$ - try all subsets
$\sqrt{n}, \log(\log n), \log^2 n$

Ordering: 
$1 << \log \log n << \log n << \log^2n << \sqrt{n} << n << n \log n << n^2 << n^3 << 2^n << n!$
Also,
$n^a \in o(n^b)$ if $b > a >0$
$\log^a n \in o(n^b) if b > 0$
$n^a \in o(2^n)$

Sometimes we will analyze an algorithm's run time in terms of several parameters. 
*Ex 1*. graph with $n$ vertices and $m$ edges. 
The run time is $O(n^2)$ but $O(n + m)$ can be better. 
*Ex 2*. input and output size. 
eg. Jarvis march $O(nh)$ where $n$ is the input size and $h$ is the output size. 

*Def'n*. For $f, g: \mathbb{N} \times \mathbb{N} \to \mathbb{R}^{\geq 0}$, 
$f(n, m)$ is $O(g(n,m))$ if there exists constants $c, n_0, m_0$ such that $f(n,m) \leq c \cdot g(n,m)$ for all $n \geq n_0$, $m \geq m_0$. 


### Lecture 3: Sep 14, 2017

Algorithmic Paradigms

1. Reduction
2. Divide and conquer
3. Greedy
4. Dynamic programming

####Reduction
Use known algorithms to solve new problems. 

**2-sum** 
Input: A[1 ... n] array of numbers and target number m. 
Find (if they exist) $i$, $j$ such that $A[i] + A[j] = m$. ($i = j$ allowed) 

Algorithm 1: $\Theta(n^2)$
```
for i = 1 .. n
	for j = i .. n
		if A[i]+A[j] = m: SUCCESS
	end
end
FAIL
```

Algorithm 2: Sort A. For each $i$ do binary search for m-A[i], 
$\Theta(n \log n) + \Theta(n \cdot \log n)$

Algorithm 3: 
First sort. For example, we have [2 3 5 11 20 22], m = 23
Start at i = 0 and j = 5. 
22 + 2 = 24 is too large, so j = j - 1
20 + 2 = 22 is too small, so i = i + 1
20 + 3 = 23 is correct. 

```
i = 1, j = n
while i <= j
	s = A[i] + A[j]
	if s > m
		j = j - 1
	else if s < m
		i = i + 1
	else SUCCESS
end
FAIL
```

Correctness invariant: if there is a solution i*, j*, then i*>= i, j* <= j. 
Run time: $\Theta(n \log n) + \Theta(n)$

**3-sum**
Input: array A[1 ... n] of numbers, target m. 
Find $i, j, k$ such that $A[i]+A[j]+A[k]=m$, allow $i = j$ etc.

Consider $m=0$
Try each $k = 1 ... n$
Find $i, j$ such that $A[i]+A[j] = -A[k]$
Run time: $\Theta(n \cdot n \log n) = \Theta(n^2 \log n)$
Notice that we only need to start $A$ once. 
So $\Theta(n \log n) + \Theta(n \cdot n)$
(Sort once, then number of $k$ times the runtime for algorithm 3 above). 

General $m$
$A'[i] = A[i] - \frac{m}{3}$
Then $A'[i] + A'[j] + A'[k] = 0 \iff A[i]+A[j]+A[k]=m$ 

Is there an algorithm better than $\Theta(n^2)$ for the worst case run time?
Yes. There has been recent developments. 

#### Divide and Conquer

Divide: break into smaller subproblems
Recurse: solve subproblems
Conquer: put subproblem solutions together

Examples: 
- binary search: $T(n) = T(\frac{n}{2}) + 1 \in \Theta(\log n)$
- merge sort: $T(n) = 2T(\frac{n}{2}) + n - 1 \in \Theta(n \log n)$

Solving recurrence relations
1. Basic approaches
	- recursion tree method
	- guess a solution, prove by induction

Example: solving merge sort recurrence

**Recursion tree method**
$T(n) = 2T(\frac{n}{2}) + c \cdot n$, T(1) = 0

For $n$ a power of 2: 
> Level 1:  $c \cdot n$
Level 2: $c \frac{n}{2} + c \frac{n}{2}  = c \cdot n$ 
Level 3: $4 (c \frac{n}{4}) =  c \cdot n$
Sum over all levels: $cn (\log_2n)$
So $\Theta(n \log n)$

For more general $n$: 
$T(n) = T(\lfloor\frac{n}{2} \rfloor) + T( \lceil \frac{n}{2} \rceil) + n - 1$, T(1) = 0
True solution: $T(n) = n \lceil \log n \rceil - 2^{\lceil \log n \rceil} + 1$

But we just care about the asymptotic behaviour. 
Most algorithms are monotonic in their worst case run time; that is, 
$T(n') \geq T(n)$ if $n' \geq n$
b
To solve for $n$ not a power of 2, just go up to the next power of 2. 
Note that $n' \leq 2n$ so at worst we double. 

**Induction**

Prove $T(n) \leq c \cdot n \log n$ by induction. 

In the base case of $n = 1$, $T(1) = 0 = c \cdot 1 \log 1$
Assume by induction $T(n') \leq c \cdot n'\log n'$ for $n' < n$. Prove for $n$. 

If $n$ is even, 

\begin{aligned}
T(n) &= 2T(\frac{n}{2}) + n - 1 \\ 
& \leq 2 \cdot c \frac{n}{2} log \frac{n}{2} + n - 1 \\
& = c \cdot n (\log n - 1) + n - 1 \\
&= c \cdot n \log n - cn + n - 1 \\
& \leq c \cdot n, \text{as long as c } \geq 1
\end{aligned}

**Some further techniques**

$T(n) = T(\lfloor \frac{n}{2} \rfloor) + T(\lceil \frac{n}{2} \rceil) + 1$ with $T(1) = 1$

Prove $T(n) \in O(n)$ by induction: 
$T(n) \leq cn$ for some $c$. 
$T(n) \leq c \lfloor \frac{n}{2} \rfloor + c \lceil \frac{n}{2} \rceil + 1 = cn + 1$

Try $T(n) \leq cn-1$
Inductive step: 
\begin{aligned}
T(n) &= T(\lfloor \frac{n}{2} \rfloor) + T(\lceil \frac{n}{2} \rceil) + 1 \\
& \leq c \cdot ( \lfloor \frac{n}{2} \rfloor -1) +  c \cdot ( \lceil \frac{n}{2} \rceil -1) + 1 \\
& = cn -1
\end{aligned}

### Lecture 4: Sep 19, 2017

####Master Theorem for Recurrences

Example: changing variables
$T(n) = 2T( \lfloor \sqrt{n} \rfloor) + \log n$
Let $m = \log n$, so $n = 2^m$. 
$T(2^m) = 2T(2^{m/2}) + m$
Let $S(m) = T(2^m)$, so $S(m/2) = T(2^{m/2})$. 
Then $S(m) = 2S(m/2) + m$ which we know $S(m) \in O(m \log m)$. 
So $T(2^m) \in O(m \log m)$, and $T(n) \in O(\log n (\log \log n)$

We often get recurrences of the form
$$T(n)=aT \left( \frac{n}{b} \right) + c \cdot n^k$$

This arises if we divide problems of size $n$ into: 
1. Sub-problems of size $n/b$
2. Do $c \cdot n^k$ extra work. 

For example: 

- Merge sort: $k=1, a=b=2$. $T(n) = 2T(n/2)+cn = O(n \log n)$. 
- $a=1, b=2: T(n) = T(n/2) + cn = O(n)$
- $a=4, b=2: T(n) = 4T(n/2) + cn = O(n^2)$

Master Theorem: 
$$T(n)=aT\left( \frac{n}{b} \right) + c \cdot n^k$$
where floor / ceiling is allowed on $\frac{n}{b}$, 
with $a \geq 1, b > 1, c > 0, k \geq 1$. 
Then, 
$$
    T(n)= 
\begin{cases}
    \Theta(n^k) ,& \text{if } a < b^k \text{ or } \log_b a < k \\
    \Theta(n^k \log n),      & \text{if } a=b^k \\
    \Theta(n^{\log_b a}),   & \text{if } a > b^k
\end{cases}
$$

\begin{align*}
T(n) &= aT(\frac{n}{b}) + c \cdot n^k \\
&= a \left( aT \left( \frac{n}{b^2} \right) + c \left(\frac{n}{b} \right)^k \right) + c \cdot n^k \\
&= a^2T \left( \frac{n}{b^2} \right) + ac \left( \frac{n}{b} \right)^k + c \cdot n^k \\
&= a^3T \left(\frac{n}{b^3} \right) + a^2c \left(\frac{n}{b^2} \right)^k + ac \left( \frac{n}{b} \right)^k + c \cdot n^k \\
&= ... \\
&= a^{\log_bn}T(1) + \sum_{i=0}^{\log_bn - 1} a^i \cdot c \cdot \left( \frac{n}{b^i} \right)^k \\
&= n^{\log_ba}T(1) + c \cdot n^k \sum_{i=0}^{\log_bn-1} \left( \frac{a}{b^k} \right)^i
\end{align*}

- If $a< b^k$, then the second term dominates, and $\sum$ is constant. So $O(n^k)$ 
- If $a=b^k$, then $\sum$ is $O(\log n)$, so get $O(n^k \log n)$
- If $a > b^k$ then the first term dominates, so $O(n^{\log_ba})$

**More general Master Theorem**

If $T(n) = aT(\frac{n}{b}) + c \cdot n^k \log^l n$ for $a \geq 1, b > 1, k \geq 0$, then
$$
    T(n)= 
\begin{cases}
    \Theta(n^k \log^ln) ,& \text{if } a < b^k \text{ or } \log_b a < k \\
    \Theta(n^k \log^{l+1} n),      & \text{if } a=b^k \\
    \Theta(n^{\log_b a}),   & \text{if } a > b^k
\end{cases}
$$

####Divide and Conquer Examples

**Counting Inversions**
If you rank $n$ movies with order $1,2,...,n$ and your friend ranks the same movies with order $i_1, ..., i_n$. The number of pairs of movies that you and your friend rank with different orders is called the inversions in the permutation $i_1, ..., i_n$. This can be used as a measure of how similar (dissimilar) you and your friend are. More formally, the number of inversions is $|\{(i_j, i_k): j <k \textrm{ and } i_j > i_k \}|$. 

Algorithm 1: For every pair of j < k, check if $i_j > i_k$ and count. O(n^2)

Algorithm 2: Adjust the merge sort algorithm

*SortAndCount*
Input: A is a list of $n$ elements. 
Output: (S, k), where $S$ is sorted, and $k$ is the number of inversions. 

1. If $n \leq 3$, sort and count with a trivial algorithm and return. 
2. $(S_1, k_1) \leftarrow \textrm{ SortAndCount } (A[1...n/2])$
3. $(S_2, k_2) \leftarrow \textrm{ SortAndCount } (A[n/2+1...n])$
4. $(S_3, k_3) \leftarrow \textrm{ MergeAndCount } (S_1, S_2)$
5. $\textrm{Return } (S, k_1 + k_2 + k_3)$

*MergeAndCount*
Input: Two sorted arrays $A$ and $B$ with length $m$ and $n$, respectively
Output: (C, k), where $C$ is sorted and $k$ is the number of inversions across $A$ and $B$.

```
i = 1, j = 1, k = 0, C = empty
while i < m and j < n
	if (A[i] < B[j]) then
		append A[i] to C
		i ++
	else
		append B[j] to C
		j ++
		k = k + m - i + 1
Append A[i,m] and B[j,n] to C
Return (C,k)
```

Proof of correctness. 
By induction. Base case is straightforward and omitted. 
Inversions in $A$ includes inversions in the first half, second half, and across the two halves. This is indeed what line 5 of SortAndCount computes. We need to ensure MergeAndCount computes the inversions across $A$ and $B$ correctly. When $A[i]$ and $B[j]$ are compared in the while loop, no inversion is encountered in the first case, and $m − i + 1$ inversions are encountered in the second case. Thus, the number of inversions is correctly computed.

Time complexity the same as merge sort: $O(n \log n)$

**Example: Closest Pair**

Input: A set of points on plain: $P = \{p_1, ..., p_n\}$. Each $p_i = (x_i, y_i)$.
 
Output: A pair of points $p_i, p_j$ that are closest among all pairs. 

We use divide and conquer. First, we need to find a vertical line that separate the points into two parts $Q$ and $R$. This can be done in linear time (find median of x-axis and then split). But to make things simpler, let’s assume we have sorted the indices of $P$ in two separate sorted arrays $P_x$ and $P_y$. This precomputation takes only $O(n \log n)$ time. After sorting, it takes $O(1)$ time to find line $L$ that separates the points into two parts, $Q$ and $R$. 

We need to ensure that $Q$ and $R$ have sorted arrays $Q_x, Q_y, R_x$ and $R_y$. $Q_x$ and $R_x$ are easy to get. To get $Q_y$ and $R_y$, we can scan through $P_y$ linearly and split the array into two while maintaining the order in $y$ axis. 

```
Q_y = empty, R_y = empty
for i from 1 to n
	if P_y[i] in Q
		append P_y[i] to Q_y
	else
		append P_y[i] to R_y
```
We can determine $P_y[i] \in Q$ through comparing x-values. 

After spending this $O(n)$ overhead to sort $Q$ and $R$, we can solve the left and right parts recursively. Without loss of generality, suppose after recursion,  the left part’s solution has distance $\delta$, which is smaller than the right part. The only remaining problem is that the closest pair may be across the two parts nearby line $L$.

If $q \in Q$ and $r \in R$ are such that $d(q, r) \leq \delta$, then they must be with in distance $\delta$ of $L$. So, we only need to limit our search within a narrow band as shown in the figure. We denote the list of points in this band by $S$. $S$ can be found in linear time by linearly scanning through the sorted P y array. Additionally, we can still have $S$ sorted according to y-axis.

Divide the plain into boxes whose length and width are $\delta / 2$. Then, each box contains at most one input point. Otherwise, the two points in the same box will have distance smaller than $\delta$, a contradiction with how $\delta$ was obtained. 

Our goal is to find a pair within distance $\delta$. For each point $q \in S$ at left side of $L$, we only need to compare it with points within $\pm 2$ boxes above and below. Thus, in the y-axis sorted list S, we only need to compare each point with the next 11 points, in order to ensure that the true answer is visited during the search. The search takes $11 \cdot |S| = O(n)$ time. If we find a pair with distance smaller than $\delta$, output that pair. Otherwise, we output the pair with distance $\delta$ found during recursion.

Time complexity: $T (n) = 2 \cdot T ( \frac{n}{2} ) + O(n)$ . Therefore $T (n) = O(n log n)$. 

**Example: Integer Multiplication (Karatsuba algorithm)**

The problem: Let $x$ and $y$ be two $n$-digit numbers. Compute $x \cdot y$. 

A straightforward algorithm would require $O(n^2)$ time to multiply each digit of $x$ with each digit of $y$. Let's try to do divide-and-conquer. 

Let $m = \frac{n}{2}$. Let $x = 2^m \cdot x_1 + x_2$ and $y = 2^m \cdot y_1 + y_2$. Then, 

$$x \cdot y = 2^{2m} \cdot x_1 y_1 + 2^m \cdot (x_1y_2 + x_2y_1) + x_2y_2$$

So we have $T(n) = 4 \cdot T(\frac{n}{2}) + O(n)$. By Master's theorem, $a=4, b=2, \textrm{ and } c = 1$. So $T(n) = n^2$. 

The trick is to reduce $a$. Notice that $(x_1+x_2)(y_1+y_2) = x_1y_1 + (x_1y_2 + x_2y_1) + x_2y_2$. So we don't need to know $x_1y_2$ and $x_2y_1$, we just need to know their sum. 

Let $r = (x_1+x_2)(y_1+y_2)$, $p = x_1y_1$, $q = x_2y_2$. Then, 
$$x \cdot y = 2^{2m}p + 2^m(r-p-q) + q$$

In other words, 

$$x \cdot y =  x_1 y_1(2^m - 1 ) 2^m + (x_1 + x_2) (y_1+y_2) + x_2 y_2 (2^m - 1)$$

Note that the multiplication with $2^m -1$ takes $O(n)$ time. So
$$T(n) = 3 \cdot T \left(\frac{n}{2} \right) + O(n)$$

Thus, by Master's theorem, $a = 3$, $b=2$ and $c=1$. So, $T(n) = n^{\log_23} < n^{1.59}$. 

Practical concerns: 
1. $n$ is odd? Two numbers have different length? (Padding?)
2. What base to use? 
3. For small $n$ overhead is too high. Better use $O(n^2)$ method. For $n > 1000$, this method is better. 
4. There is an $O(n \log n \log \log n)$ method that is asymptotically better but requires very large $n$ to be beneficial. 

**Example: Matrix Multiplication (Strassen Algorithm)**
Let $A$ and $B$ be two matrices, $C = A \times B$, then $c_{i,j} = \sum^n_{k=1}a_{i,k}b_{k,j}$. For brevity we assume each matrix is $n \times n$ matrix. A straightforward algorithm would take $O(n^3)$ time. 

We can divide each matrix into sub-matrices of size $n/2$ as follows: 

\begin{equation}
\begin{pmatrix} C_{1,1} & C_{1,2} \\ C_{2,1} & C_{2,2} \end{pmatrix}
 = \begin{pmatrix} A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2} \end{pmatrix} \times
\begin{pmatrix} B_{1,1} & B_{1,2}\\ B_{2,1} & B_{2,2} \end{pmatrix}
\end{equation}

Then, 
\begin{align*}
C_{1,1} &= A_{1,1}B_{1,1} + A_{1,2}B_{2,1} \\
C_{1,2} &= A_{1,1}B_{1,2} + A_{1,2}B_{2,2} \\
C_{2,1} &= A_{2,1}B_{1,1} + A_{2,2}B_{2,1} \\
C_{2,2} &= A_{2,1}B_{1,2} + A_{2,2}B_{2,2}
\end{align*}

Using these straightforwardly will not make computations faster. Instead, Strassen computed seven other matrices: 

\begin{align*}
M_1 & := (A_{1,1} + A_{2,2})(B_{1,1} + B_{2,2}) \\
M_2 & := (A_{2,1} + A_{2,2}) B_{1,1} \\
M_3 &:= A_{1,1} (B_{1,2} - B_{2,2}) \\
M_4 & := A_{2,2} (B_{2,1} - B_{1,1}) \\
M_5 &:= (A_{1,1} + A_{1,2}) B_{2,2} \\
M_6 & := (A_{2,1} - A_{1,1}) (B_{1,1} + B_{1,2}) \\
M_7 &:= (A_{1,2} - A_{2,2}) (B_{2,1} + B_{2,2})
\end{align*}

It can be verified that 
\begin{align*}
C_{1,1} &= M_1 + M_4 - M_5 + M_7 \\
C_{1,2} &= M_3 + M_5 \\
C_{2,1} &= M_2 + M_4 \\
C_{2,2} &= M_1 - M_2 + M_3 + M_6
\end{align*}

Matrix additions take only $O(n^2)$ time. So, $T(n) = 7T(\frac{n}{2}) + O(n)$. By using Master's Theorem, the time complexity is $T(n) = O(n^{\log_2 7}) \approx O(n^{2.807})$. 

**Exercise: Polynomial Multiplication**
Given two polynomials $p(x)$ and $q(x)$, compute $p(x) \cdot q(x)$. In this problem we assume numeric summation and multiplication takes constant time. The idea is very similar to integer multiplication. 
