
Outline
* Introduction to Computer Security and Privacy (1.5 hours)
    The meaning of computer security; comparing security with privacy; types of threats and attacks; methods of defense
* Program Security (6 hours)
    Secure programs; nonmalicious program errors; malicious code; controls against program threats
* Operating System Security (6 hours)
    Methods of protection; access control; user authentication
* Network Security (4.5 hours)
    Network threats; firewalls, intrusion detection systems
* Internet Application Security and Privacy (9 hours)
    Basics of cryptography; security and privacy for Internet applications (email, instant messaging, web browsing); privacy-enhancing technologies
* Database Security and Privacy (4.5 hours)
    Security and privacy requirements; reliability, integrity, and privacy; inference; data mining; k-anonymity
* Non-technical Aspects (4.5 hours)
    Administration of security systems; policies; physical security; economics of security; legal and ethical issues

It is expected that you will have read the appropriate sections of the textbook (as noted in the Modules section of LEARN) before class. Additional readings may be assigned as well, and will appear on the lecture slides page; those readings marked as mandatory contain required material for the course; those marked before class must be read before the date of the corresponding lecture.

# Module 1

What is security? 
A computer system is said to be secured if it has the three properties: 

* Confidentiality: access limited to authorized parties
* Integrity: The "right" data is received
* Availability: System or data is there when you want it

You can rely on a secured system to: 
* keep personal data confidential
* allow only authorized access or modifications to resources
* ensure that any produced results are correct
* give you correct and meaningful results whenever you want it

What is privacy? One useful definition is "informational self-determination": 
* you get to **control** information about you: 
    * who gets to see it
    * who gets to use it
    * what they can use it for
    * who they can give it to, etc...

Assets: Things we want to protect, such as hardware, software, data

Vulnerabilities: Weaknesses in a system that may be able to be *exploited* in order to cause loss or harm
* eg. a file server that doesn't authenticate its users

Threats: loss or harm that might befall a system; 4 types:
1. Interception: some unauthorized party has gained access to asset; data file copied, wiretapping to obtain data from network
2. Interruption: asset becomes lost, unavailable, or unusable; eg. malicious destruction of device
3. Modification: modifed asset; eg. change value in database, alter program
4. Fabrication: insert transactions to network; add records to databases

When designing a system, we need to state the threat model: 
* sets of threats we are undertaking to defend against
* whom do we want to prevent from doing what?

Attack: an action that exploits a vulnerability to execute a threat

Control/Defence: Removing or reducing a vulnerability
* goal: control vulnerability to prevent an attack or defend against a threat

Methods of defence: 
* Prevent it
* Deter it: make attack harder or more expensive
* Deflect it: make yourself less attractive to attackers
* Detect it: notice when attack is/has occurred
* Recover from it: mitigate effects of attack

We want to do many things to defend against the same threat - "Defence in depth"

How secure should we make it? 
* Principle of easiest penetration: a system is only as strong as its weakest link
* Principle of Adequate Protection: Security is economics

**Ways to prevent assets**

Cryptography
* data unreadable to an attacker
* digital signatures, cryptographic protocols
* ensure integrity of stored data 

Software controls
* password or other form of access controls
* OS separate users' actions from each other
* virus scanners
* development controls to ensure quality measures on original source code
* personal firewalls

Hardware controls
* Fingerprint readers, smart tokens, firewalls, intrusion detection systems

Physical controls
* Locks, guards, off-site backups

Policies and procedures
* non-technical means used to protect against certain classes of attack
* eg. prevent employee from connecting their own wifi access point to internal company network
* rules about choosing passwords
* training in security practices

# Module 2

## Flaws, faults and failures

Flaw: problem with a program

Security flaw: problem that affects security in some way (confidentiality, integrity, availability)
* faults: potential problem; error in code,data, specification, process, etc.
    * programmer / specifier / inside view
* failure: deviation from desired behaviour
    * user / outside view

To discover and fix faults: 
* When user experiences a failure, we can work backwards to uncover the underlying fault. 
* Intentionally try to cause failures
* Faults are fixed by making small edits to the program- "penetrate and patch"

Problems with patching: 
* pressure to patch a fault is often high, causing programmer to focus on observed failure rather than looking at what may be a more serious underlying problem
* fault may have caused other, unnoticed faults; partial fix cause inconsistencies or other problems
* patch may introduce new faults

Unexpected behaviours: 
* when program's behaviour is specificed, spec lists the things the program must do
* most implementors wouldn't care if it did additional things as well
* from security / privacy point of view, extra behaviours can be bad
* when implementing security or privacy relevant program, add "and nothing else" to the spec

Types of security flaws
* Intentional / inherent
    * malicious: intentionally asserted to attack systems
        * targeted: to attack a particular system
        * nontargeted
    * nonmalicious: features that can cause failure when used by attacker
* unintentional

## Unintentional security flaws

TLS Heartbeat mechanism: designed to keep SSL/TLS connections alive even when no data is being transmitted. Heartbeat messages sent by one peer containing random data and a payload length, the other peer is supposed to respond with mirror of exactly the same data. 

Heartbleed Bug in OpenSSL (2014)
* Missing bound check in the code
* attacker request that a TLS server hand over relatively large slice of its private memory space
* memory space stores server's private key material and TLS session keys

Apple's SSL/TLS Bug (2014)
* bug occurs in code used to check validity of server's signature on a key used in SSL/TLS connection
* active attacker could exploit this flaw to get a user to accept a counterfeit key chosen by the attacker

```C
static OSStatus
SSLVerifySignedServerKeyExchange(SSLContext *ctx, bool isRsa, SSLBuffer signedParams, uint8_t *signature, UInt16 signatureLen)
{
    OSStatus        err;
    ...

    if ((err = SSLHashSHA1.update(&hashCtx, &serverRandom)) != 0)
        goto fail;
    if ((err = SSLHashSHA1.update(&hashCtx, &signedParams)) != 0)
        goto fail;
        goto fail;
    if ((err = SSLHashSHA1.final(&hashCtx, &hashOut)) != 0)
        goto fail;
    ...

fail:
    SSLFreeBuffer(&signedHashes);
    SSLFreeBuffer(&hashCtx);
    return err;
}
```

Second `goto fail` statement always execute if the first two checks are successful. The third check is bypassed and 0 is returned as the value of error. 

Types of unintentional flaws: 
* buffer overflows
* integer overflows
* format string vulnerabilities
* incomplete mediation
* TOCTTOU erros

### Buffer overflows

```c
#define LINELEN 1024
char buffer[LINELEN];
gets(buffer);
    //or
strcpy (buffer, argv[1]);
```

The `gets` and `strcpy` functions do not check that the string will fit in the buffer. 

If the attacker can write data past the end of an array on the stack, she can overwrite things like the saved return address - when the function returns, program will jump to any address of her choosing. 

Targets programs on local machine that run with setuid (superuser) privileges, or network daemons on a remote machine

Variants: 
* attacks which work when a single byte can be written past the end of the buffer (caused by off-by-one error)
* overflows of buffers on heap instead of stack
* jump to other parts of program, or parts of standard libraries, instead of shellcode

Defence: 
* Programmer: use language with bounds checking and catch the exceptions
* Compiler: place padding between data andreturn addresses (canaries)
    * detect if stack has been overriden before return from each function
* Memory: non-excutable stack
    * memory page is either writable or executable, but never both
* OS: Stack (and sometimes code, heap, libraries) at random virtual addresses for each process

### Integer overflows
* Program assumes integer is always positive, overflow will make signed integer wrap and become negative, violated assumption
    * casting of large unsigned integer to signed integer
    * result of mathematical operation
* attacker passvalues to program that triggers overflow

### Format string vulnerabilities

* Unfiltered user input is used as format string in `printf()`, `fprintf()`, `sprintf()`
* `printf(buffer)`instead of `printf("%s", buffer)`
    * parse buffer for %'s and usewhatever is currently on th estack to process found format parameters
* `printf("%s%s%s%s")` likely crashes program
* `printf("%x%x%x%x")` dumps part of the stack
* `%n` write to address found on stack

```c
char output[44];
char buffer[44];
snprintf(buffer, sizeof(buffer), "Input %s", input);
sprintf(output, buffer);
```
* what happens When input=%48d+(address of a libc routine)? 

### Incomplete mediation
* inputs to programs often specified by unstrusted users
* users sometimes mistype data in webform
* mediation ensures what user has entered constitues a meaningful request
* incomplete mediation: application accepts data from the user
* focus on catching entries clearly wrong: 
    * not well formed
        * DOB: 1980-04-31
    * unreasonable values: 
        * DOB: 1876-10-12
    * inconsistent with other entries
* make sure any user-specified input falls within well-specified values, known to be safe
* concerns examples: 
    * SQL injection - student name is '); DROP TABLE Students;--
    * data causing buffer overflow

Client-side mediation: Javascript code will first run validation checks on the data entered; if you enter invalid data, popup will prevent you from submitting it

Many web sites rely on client to keep state for them. They put hiddenfields in the form which are passed back to the server when the user submits the form. 

Client-side mediation is an OK method to use in order to have a friendlier user interfae, but is useless for security purposes

If user: 
* turns off Javascript
* Edits form before submitting it (Greasemonkey)
* Writes a script that interacts with the web server instead of using a web browser
* Connects to server manually (telnet server.com 80)

Now the user can send arbitrary, unmediated values to the server. The user can also modify client-side state. 

Example: 

1. At bookstore website, user orders a book. Server replies with a form asking the address to ship to, the form has hidden field storing the user's order

    ```html
    <input type="hidden" name="isbn" value="0-13-239077-9"> 
    <input type="hidden" name="quantity" value="1">
    <input type="hidden" name="unitprice" value="111.00">
    ```

    User can change unitprice to "50.00" before submitting the form

2. Change quantity to "-1" to be paid. 


Defences against incomplete mediation: 
* Server-side mediation
* For user input: 
    * Careful check on all values on all field
    * values can contain completely arbitrary 8-bit data and be of any length
* For state stored by client: 
    * make sure client has not modified data in any way

### TOCTTOU errors

TOCTTOU (Tock-too) errors: Time-of Check to Time-of Use; also known as race condition errors

Occur when
1. user requests system to perform an action
2. system verifies the user is allowed to perform the action
3. system performs the action

The state of the system changes between step 2 and 3. 

Example: Setuid

* runs with superuser privileges so it can allocate terminals to users (a privileged operation)
* Supports a command to write contents of terminal to log file
    1. Checks if user has permission to write to requested file
    2. Open file for writing
* Attacker makes a symbolic link:   
    `logfile -> file_she_owns`
* between "check" and "open", she changes it   
    `logfle->/etc/passwd`

Defence: When performing a privleged action on behalf of another party, make sure all information relevant to access control decision is constant between time of check and time of action: 
* keep a private copy of request itself so that request can't be altered during the race
* whenever possible, act on object itself, not some level of indirection
    * eg. make decisions based on file handles, not file names
* if not possible, use locks to ensure object is not changed during race

## Malicious code: Malware

Malware is written with malicious intent. They need to be executed in order to cause harm. 

Malware can get executed by: 
1. User action
    * downloading and running malicious software
    * viewing web page with malicious code
    * opening executable email attachment
    * inserting CD/DVD or USB flash drive
2. Exploiting existing flaw in a system
    * buffer overflows in network daemons
    * buffer overflows in email clients or web browsers

Type of malware
1. Virus: 
    * add itself to benign programs/files
    * code for spreading + code for attack
    * usually activated by user
2. Worms
    * malicious code spreading with no or little user involvement
3. Trojans
    * malicious codehidden in seemingly innocent program that you download
4. Logic bombs:
    * malicious code hidden in programs already on your machine

### Virus

Virus infects other files
* executable programs
* data documents with executable code (macros)

When file is executed (or sometimes just opened), virus activates, and tries to infect other files with copies of itself to spread

Infection: 
* modify existing program or document (host) such that executing or opening it will transfer control to virus
    * for executable programs, virus modify other programs and copy itself to beginning of targets' program code
    * for document with macros, virus edit other documents to add itself as a macro which start automatically when file is opened
* often try to infect computer itself to automatically activate when computer is booted
    * put in boot sector of hard disk
    * add to list of startup programs
    * infect one of more programs OS runs at boot time

Spreading: 
* by users sending infected files to others, or putting them in p2p network
* usually require user action (otherwise it's usually a worm)

Virus try to evade detection by disabling active virus scanning software

Virus carries payload, which will activate at some point to: 
* erase hard drive
* sbutly corrupt some spreadsheets
* install keystroke logger to capture online banking password
* start attacking a particular target website

Look for virus when: 
* files added to computer
* scan entire state of computer from time to time

Look for virus using
1. signature-based protection
    * keeps a list of known viruses. For each virus, store the signature - characteristic features
        * features of virus code - infection code or payload code
        * patterns of virus - where on system it hides, how it propagates from one place to another

    To evade signature based virus scanners, some viruses are polymorphic - it makes a modified copy each time
    * most of virus code encrypted
    * virus starts with decryption routine
    * when virus spreads, it encrypts the new copy wiht a newly chosen random key

2. behaviour-based protection
    * look for suspicious patterns of behaviour; sometimes run them in sandbox first

False negatives vs. false positives?
* base rate= rate of virus in all programs
* many more false positives than true positives, potentially causing true positives to be overlooked or scanner disabled. 

### Worms

Worm is a self-contained piece of code that can replicate with little or no user involvement. Typically: 
1. Exploits security flaw in widely deployed software on computer
2. Search for other computers on local network or internet to infect
3. May have payload that activates at certain time, or by another trigger

Morris worm (1988): Once infected, a machine try to infect other machines by: 
* exploit a buffer overflow in "finger" daemon
* use a backdoor left in the "sendmail" daemon
* try a dictionary attack against local users' passwords. Success => login, spread to other machines they can access without requiring a password

Code Red worm (2001): Exploit buffer overflow in Microsoft's ISS web server. Infected machine would: 
* deface its home page
* launch attacks on other web servers
* launch a denial-of-service attack on some websites, including www.whitehouse.gov
* install backdoor to deter disinfection

Slammer worm (2003): performed denial-of-service attack
* first example of "Warhol worm" - can infect nearly all vulnerable machines in 15 minutes
* exploited buffer overflow in Microsoft's SQL server
* vulnerable machine infected with a single UDP packet
    * enables worm to spread extremely quickly
    * exponential growth

Stuxnet (2010)
* Allegedly created by US and Israeli intelligence agencies
* Allegedly targeted Iranian uranium enrichment program
* Targets Siemens SCADA systems installed on Windows; application: operation of centrifuges
* uses many criteria to select which systems to attack after infection
* promiscuous: used 4 different zero-day attacks to spread; installed manually for air-gapped systems
* stealthy: intercepts commands to SCADA system and hides its presence
* targeted: detects if variable-frequency drives are installed, operating between 807-1210 Hz, then subtly changes the frequencies so that distortion and vibrations occur resulting in broken centrifuges

WannaCry (2017)
* ransomware - demand ransom to return hostage resource to victim, can also be scareware
* exploits a Windows SMB vulnerability originally discovered by NSA, who kept it secret
* vulnerability leaked by "Shadow Brokers" in April 2017
* Microsoft released a patch, but many systems remained unpatched

CryptoLocker (2013)
* spread with spoofed email attachment from a botnet
* encrypted victim's hard drive
* demanded ransom for private key

### Trojan Horses
* programs which claim to do something innucuous, and also hide malicious behaviours
* get user to run code of attacker's choice, by providing some code user wants to run
    * PUP (potentially unwanted programs)
    * scareware: user may pay attacker to run the code
* payload can be anything
* Trojan horses do not spread themselves between computers, but rely on users to execute or share the software

### Logic Bombs
* malicious code hiding in software already on computer, waiting for a trigger to execute its payload
* written by insiders, meant to be triggered in the future (eg. when insider leaves company)
* payload is usually dire
    * erase data, corrupt data, encrypt data and ask for money
* trigger is usually something the insider can affect once he is no longer an insider
    * when a particular account gets 3 deposits of equal value in one day
    * when a special sequence of numbers is entered on the keypad of an ATM
    * time bomb

Trojan horses and logic bombs are hard to spot because user is intentionally running the code. Countered by preventing the payload from doing bad things. 

## Other Malicious code

* web bugs (beacon)
* back doors
* salami attacks
* privilege escalation
* rootkits
* keystroke logging
* interface illusions


### Web Bugs

* object (usually on 1x1 pixel transparent image) embedded in a web page, which is fetched from a different server than the one that served the web page itself. 
* Information is sent without your knowledge to third parties (often advertisers): 
    * IP address
    * contents of cookies
    * personal info the site has about you
* issue of privacy: instructs browser to behave in a way contrary to principle of informational self-determination

```html
<IMG WIDTH="1" HEIGHT="1" 
src="http://app.insightgrit.com/1/nat?
id=79152388778&ref=http://www.eff.org/
Privacy/Marketing/web bug.html&z=668951
&purl=http://quicken.intuit.com/">
```

* With help of cookies, advertiser can learn what websites a person is interested in. 
* Advertiser can learn person's identity if he can place ads on social networking site
* HTTP request for Facebook ad:   
    GET [pathname of ad]  
    Host: ad.doubleclick.net  
    Referer: http://www.facebook.com/    
    profile.php?id=123456789&ref=name   
    Cookie: id=2015bdfb9ec...   

### Back doors

Also called trapdoor, is a set of instructions designed to bypass normal authentication mechanism and allow access to system to anyone who knows the back door exists. 

Real examples: 
* debugging back door left in sendmail
* bacdoor planted by Code Red worm
* port knocking: system listens for connection attempts to a certain pattern of (closed) ports. All connection attempts will fail, but with right pattern, system will open, for example, a port with root shell attached to it
* attempted hack to Linux kernel source code: 
    ```c
    if ((options == (__WCLONE|__WALL)) && (current->uid=0))
        retval = -EINVAL;
    ```

Sources of back doors
* forget to remove them
* intentionally left for 
    * testing purpose
    * maintenance purposes
    * legal reasons
    * malicious purposes

### Salami attacks
* attack made up of many smaller, often considered inconsequential, attacks
* example: 
    * send fractions of cents of round-off error from many accounts to a single account owned by the attacker
    * credit card thieves make very small charges to many cards
    * clerks slightly overcharge customers for merchandise
    * gas pumps misreport amount of gas dispensed

### Privilege escalation 

An attack that raises privilege level of attacker. Source: 
* part of system that legitimately runs with higher privilege tricked into executing commands (with higher privilege) on behalf of the attacker
* attacker trick system into thinking he is a legitimate higher-privileged user
    * problems with authentication system
    * obtain session id/cookie from another use to access their bank account

Vertical privilege escalation: user or process can access a higher level of access than an adimitrator or system developer intended; eg. performing kernel-level operation

Horizontal privilege escalation: application allows attacker to gain resources usually protected from an application or user; eg. by impersonating another user

### Rootkits

Tool used by script kiddies. 

Has two main parts: 
* method for gaining unauthorized root / admin privileges on a machine
    * exploit some known flaw in the system
    * leaves behind a back door so attacker can get back in later
* a way to hide its existence
    * clean up log message that might have been created by the exploit
    * modify commands like `ls` or `ps` so they don't report files or processes belonging to the rootkit
    * modify kernel so no user program will learn about these files or processes

Example: Sony XCP
* Sony audio CDs equipped with XCP - extended copy protection
* when inserting CD, it contains an `autorun.exe` file that automatically executes to install the rootkit
* rootkit modify the CD driver in Windows so that nay process that tried to read the contents of an XCP-protected CD into memory would get garbled output
* rooknit was hard to find and uninstall
* Sony released an uninstaller after complaints and law suits, but the uninstaller left a back door

### Keystroke Logging

Attacker may install keyboard logger on computer to keep record of 
* emails / IM sent
* password typed

The data can then be accessed locally or sent to a remote machine over the Internet. 

Installed by: 
* malware
* family member to spy on children, spouses, etc.

Types: 
* Application-specific Loggers: record keystrokes associated with a particular application
* System keyboard loggers: record all keystrokes that are pressed (maybe only for a particular target user)
* Hardware keyboard loggers: sits between keyboard and computer; 
    * works with any OS and undetectable in software

### Interface Illusions

User interface gives unwanted (nonstandard) side effects. 

Example: dragging scrollbar dragged a program from a malicious website into your Startup folder, in addition to scrolling the document

**Phishing**: Make a fake website look like the real thing. 

Phishing detection: 
* unusual email / URL
* attachments with uncommon names
* typos, unusual wordings
* no https

Keyboard logging, interface illusions, and phishings are all examples of **Man-in-the-middle attacks**
* The webiste/program/system you're communicating isn't the one you think you are communicating with
* intercepts communication from user, then passes it onto the intended other party
    * user thinks nothing is wrong, because expected behaviours are observed
* man-in-the-middle can hijack session to insert malicious command, once , for example, you've authenticated to your bank
* can edit results (eg. bank balances) so there's no visible record

## Nonmalicious flaws

Covert channels
* creates capability to transfer sensitive / unauthorized information through channel that is not supposed to transmit that info
    * what info can be transmitted may be determined by policy, guidelines, physical limitations, etc.
* eg. by using binary bit stream

Side channels
* attack based on information gained from implementation of a computer system
* carefully watches how the system behaves
* usually has to be somewhere in the physical vicinity
* potential attack vectors: 
    * bandwith consumption
    * shoulder-surfing
    * reflection of screen
    * timing computations
    * power consumption
    * electromagnetic emission
    * sound emission
    * cache access
    * differential power analysis
    * differential fault analysis

## Controls against security flaws in programs

Software goes through several stages in its lifecycle:
- Specification
- Design
- Implementation
- Change management
- Code review
- Testing
- Documentation
- Maintenance

Design programs to have less security flaws
- modularity
    - each module responsible for single subtask, easier to check for flaws, test, maintain, reuse, etc.
    - low coupling (interactions between modules)
- encapsulation
    - modules mostly self-contained, sharing information only as necessary
    - reduces coupling
    - developer of one module does not need to know how a different module is implemented - she should only know about published interfaces (API)
- information hiding
    - internals of one module not visible to other modules
    - implementation and internal state not modifiable except for API
- mutual suspicion
    - modules check that their inputs are sensible before acting on them
    - defend against flaws or malicious behaviour on the part of other modules
        - corrupt data in one module should be prevented from corrupting other modules
- confinement
    - if module A needs to call potentially untrustworthy Module B, sandbox it
        - run in a limited environment that only has access to resources it absolutely needs
    
Implementation phase: 
* Don't use C if possible
* static code analysis
    * software products that help you find security flaws- buffer overflow, TOCTTOU, etc.
* formal methods
    * prove code does exactly what it's supposed to do
    * impossible to do in general
* genetic diversity
    * worms and viruses can spread because many machines run the same vulnerable code
    * more variety -> smaller likelihood of common flaw

Source control and configuration control
- track all changes to either source code of configuration information in somemanagement system
- in attempted backdoor in Linux source, Bitkeeper notice change to source repo that didn't match any valid checkin

Code review
- have people look at code to try to find flaws
- guided code reviews: author explains code to reviewer
    - why each change was made
    - what effects it might have on other parts of the system
    - what testing needs to be done
- author inserts intentional flaws into code - reviewers will look harder

Testing phase: 
- try to make programers do unspecified things
- black box testing
    - fuzz testing: supply completely random data to the object, as
        - input in API
        - data file
        - data received from network
        - UI events
- white/clear box testing
    - regression testing

Documentation
- document choices that worked and didn't work
- make checklist of things to be careful of

Standards, process, and audits
- standards: rules about how things are done at each stage of software lifecycle; incorporate controls
    - what design methodologies?
    - what kind of implementation diversity?
    - what change management system?
    - what kind of code review?
    - what kind of testing?
- formal processes specify how each standard should be implemented
- audits: external personnel comes in and verifies that processes are followed properly


# Module 3: Operating System Security

1. Protection in general-purpose operating systems
2. Access control
3. User authentication
4. Security policies and models
5. Trusted operating system design


## Protection in general-purpose operating systems

Protected objects: 
* Memory
* Data
* CPU
* Programs
* I/O devices (disks, printers, keyboards, ...)
* Networks
* OS

Separation is used to keep one user's objects from other users. 
* Physical separation: Different physical resources for each user; easy to implement, but expensive and inefficient
* Temporal separation: execute different users' programs at different times
* Logical separation: OS gives users an impression that no other users exist
* Cryptographic separation: encrypt data and make it unintelligible to outsiders

Flexible sharing is needed to share resources (library routines, files). 

Memory and address protection 
* Prevents one program from corrupting other programs, OS, or itself
* OS can exploit hardware support for protection
* Memory protection is part of translation from virtual to physical addresses
    * MMU generates exception if something is wrong
    * OS maintains mapping table used by MMU and deals with raised exceptions

Protection techniques
1. Fence register
    * check if memory access below address in fence register
    * protects OS from user programs; single user only
2. Base/bounds register pair
    * one pair for each user program
    * checks if memory access below/above address in base/bounds register
    * maintained by OS during context switch
    * limited flexibility
3. Tagged architecture
    * each memory word has extra bit(s) that identify access rights to word
    * very flexible, but large overhead
    * difficult to port between hardware architectures
4. Segmentation
    * each program has different address spaces (segments) for code, data, stack with different access restrictions
    * virtual address: \<segment name, offset with segment\>
    * OS keeps "segment table" for each process to map segment name to base physical address; segment table has protection attributes
    * OS can relocate or resize segments and share them between processes

    Advantages: 
    * Each address reference checked for protection by hardware
    * different classes of data items can be assigned different levels of protetcion
    * users can share access to segment with different access rights
    * user cannot access unpermitted segment

    Disadvantages: 
    * external fragmentation (free memory is in small blocked interspersed between allocated memory; too small to satisfy demands)
    * dynamic length of segments requires costly out-of-bounds check for generated physical address
    * segment names are difficult to implement efficiently
5. Paging
    * Virtual address space divided into equal-sized chunks (pages)
    * Physical memory divided into equal-sized chunks (frames)
    * frame size = page size
    * virtual address: <page #, offset within page>
        * \# of bits for offset = log_2(page size)
    * OS keeps Page Table - maps page \# to base physical address
    * page table has memory protection attributes

    Advantages: 
    * address reference checked for protection by hardware
    * user can share access to page with different access rights
    * user cannot access unpermitted page
    * unpopular pages can be moved to disk to free memory

    Disadvantages: 
    * internal fragmentation
    * assigning different levels of protection to different classes of data items not feasible

x86 architecture
* has both segmentation and paging
* memory protection bits indicates: no access, read/write access, read-only access
* most processors also include NX (No execute) bit, forbidding execution of instructions stored in some page; eg. make stack / heap non-executable

## Access Control

Three goals: 
* check every access
* enforce least privilege: grants program the smallest number of objects required to perform a task
* verify acceptable use: limit types of activity that can be performed on an object

Access control structures: 
* Access control matrix
* Access control lists
* Capabilities
* Role-based access control

Access control matrix
* Has
    * Set of protected objects: O (files, database records)
    * Set of subjects: S (users, processes)
    * Sets of rights: R (read, write, execute, own)
* Consists of entries a[s,o] where s \in S, o \in O and a[s,o] \in R
* Typically implemented as
    * set of access control lists (column-wise representation)
    * set of capabilities (row-wise representation)
    * or a combination

Access control lists (ACLs)
* Each object has list of subjects and their access rights
* Implemented in Windows file system (NTFS), user entry can denote entire user group ("Students")
* UNIX file system has simple ACLs. Each file lists its owner, group, and "others". 
    * group defined system-wide in /etc/group
    * chmod/chown/chgrp for setting rights

Capabilities
* unforgetable token that gives its owner some access rights to an object
* OS store andmaintain tokens or by cryptographic mechanisms
    * eg. digital signatures allows tokens to be handed out to processes/users; OS detect tampering when process/user tries to get access with modified tokens
* tokens may be transferable
* some research OSs have support for tokens

Combined usage of ACLs and cap.
* In UNIX, each file has ACL, which is consulted when executing an `open()` call
* if approved, caller given a capability listing type of access allowed in ACL (read/write)
    * capability stored in memory space of OS
* Upon `read()` or `write()`, OS look at capability to determine type of access allowed

Role-based access control (RBAC)
* objects a user can access often depend on user's job function (role)
* in RBAC, administrator assigns user to roles and grant access rights to roles
* when user takes new role, needto update only role assignment
* used in commercial database
* supports complex access control scenarios: 
    * Hierarchical roles: "manager is also an employee"
    * users can hae multiple roles and assume/give up roles as required by current task
        * manager for A, tester for B
        * user's current session contains currently initiated role
    * separation of duty: "payment order needs to be assigned by a manager and an accounting person, where two cannot be the same person"

## User Authentication

Computer systems need to identify and authenticate users before authorizing them
* Identification: who is user?
* authentication: prove it

4 classes of authentication factors: 
* knows: password, pins
* has: ATM card, badge, smartphone
* is: biometrics
* context: location, time, devices in proximity

Different classes can be combined for more solid authentication.   

"have" can turn into "know": 
* duplicate tokens
* token displays number that change over tme
* sms message

Password
* usability: inconvenience; composition; forgotten password; shared password
* security: shoulder surfing, keystroke logging, interface illusions, password re-use, password guessing
* password guessing attacks: 
    * brute force
    * using common patterns, dictionaries
    * offline attack: requires attack to have encrypted file
    * online attack: can be detected by shutting down access after a few attempts
* password hygiene
    * password manager
    * pass phrase
    * site specific password
    * don't disclose passwords
    * don't enter on public computer
* advice for developers
    * usability / friendliness
    * security

Attacks on password files
* store only digital fingerprint of password using cryptographic hash in password file
* system compute fingerprint of entered password, compares it with stored fingerprint
* still allows offline guessing attacks when password file leaks

Defends against guessing attacks: 
* User specific salt in password fingerprint - used by UNIX
    * initially derived from time of day and process ID of /bin/passwd
    * stored in password file in plaintext
    * users with same password will likely have diff fingerprints
    * attackers can't just build single table of fingerprints and passwords and use it for any password file
* Don't use standard cryptographic hash (SHA-1, SHA-512), relatively cheap to compute - microseconds
    * use iterated hash function that is expensive to compute (bcrypt) and maybe use lots of memory (scrypt) - hundreds of milliseconds
    * slows down guessing attack, but not noticed when users enters password
* use MAC instead of cryptographic hash
    * mixes in secret key to compute passwd fingerprint
    * needs secret key for guessing attacks; 
    * protects key by embedding it in tamper resistant hardware
    * if key leaks, scheme remains as secure as cryptographic hash

Password recovery
* password cannot be recovered from hash value
* need to store encrypted version of password in password file
* allows system to easily re-compute a password if needed

Adobe Password Hack (Nov 2013)
* 130 million encrypted passwords for Adobe accounts were revealed: 
* Encrpytion mechanism: 
    1. NUL byte appended to password
    2. Additional NUL bytes appended as reuiqred to make length a multiple of 8 bytes
    3. Padded password were encrypted 8 characters at a time using a fixed key (ECB mode - weakest possible encryption mode)
* password hint not encrypted
* many passwords can be decrypted without breaking the encryption and not knowing the key

Interception attacks
* intercepts password while it's in transmission from client to server
* defense: challenge-response protocols
    * server sends random challenge to client
    * client uses challenge and password to compute one-time password
    * client sends one-time password to server
    * server checks whether client's response is valid
* attacker may be able to brute-force password using intercepted challenge and response
* cryptographic protocols (eg. SRP) can make intercepted info useless
* on web, password transmitted mostly in plaintext
    * sometimes with digital fingerprint
    * encryption (TLS) protects against interception attacks on network
    * alternative solutions difficult to deploy
    * don't help against interception on client side (malware)



Server authentication
* system authentications user with password
* user should authenticate system (server) too! 
* Phishing: eg. fake login screen
    * Windows uses <CTRL-ALT-DELETE> for login because key combination cannot be overriden by attacker

Biometrics: 
* accept if observed trait sufficiently close to stored trait
    * false positives: Alice accepted as Bob  
    * false negative: Alice rejected as Alice
* works well for local authentication, but not remote
* Authentication vs Identification: 
    * authentication: does captured trait respond to particular stored trait?
    * identification: does captured trait respond to any of stored traits? 
        * expensive search problem
* Identification problem: Many false alarms due to false positives
* Privacy: difficult to replace biometric info in case of leak
* Accuracy: no authenticate in the case of false negatives
* Secrecy: some biometric info are not secret; eg. face
* legal protection: law may protect you from forced password authentication, but not biometric authentication

## Security policies and models

Trusting an entity means that if entity fails, security fails

We trust an OS if we have *confidence* that it provides security services; i.e. memory and file protection, access control and user authentication

Trusted OS builds on 4 factors: 
* Policy: set of rules outlining what is secured and why
* Model: implements policy and can be used for reasoning about policy
* Design: specification of how the OS implements the model
* Trust: Assurance that the OS is implemented according to design

Trusted software: 
* rigorously devleoped so the code does what it is expected and nothing more. 
1. functional correctness
2. enforcement of integrity: wrong input don't impact data correctness
3. limited privilege
4. appropriate confidence level: software rated as required by environment

Confidentiality (Security) Policies
* Many OS security policies are rooted in military security policies
* Each object has a sensitivity / clearance level: Top Secret > Secret > Confidential > Unclassified
* Each object may be assigned to one or more compartments
* "need-to-know" restriction: subject must has a need to access object o in order to access it, even if it has the security clearance
* subject can access object o iff s dominates o, or level(s) >= level(o) and compartments(s) \contains compartments (o)

Other policies: 
* Integrity of information: eg. Clark-Wilson Security Policy
    * based on well-formed transactions that transition system from consistent state to another
    * supports separation of duty
* conflicts of interests: eg. Chinese Wall Security Policy
    * once you have been able to access info about one kind of company, you can no longer access info about other companies of same kind
        * needs history of accessed objects
        * access rights change overtime
    * ss-property: subject s can access object o iff each object previously accessed by s either belongs to the same company as o or belongs to a different kind of company than o does
    * \*-property: for a write access to o by s, we also need to ensure that all objects readable by s either belong to the same company as o or have been sanitized
        * prevents indirect information flow, where A access info about B (that he cannot access directly) from a source C
        * in that case, D cannot write that info to C

Lattices: 
* Dominance relationship defined in military security model is transitive and antisymmetric
    * defines partial order
* in a lattice, for every a and b, there is a unique lowest upper bound u for which u dominates a and u dominates b, and a unique greatest lower bound l for which a dominates l and b dominates l. 
* There are two elements U and L that dominate / are dominated by all levels

Bell-La Padula confidentiality model
* regulates information flow in MLS policies, e.g. lattice-based ones
* users get info onlyaccording to clearance
* information can only flow up
* should subject s with clearance C(s) have access to object with sensitivity C(o)?
* ss-property("no read up"): s should have read access to o only if C(s) >= C(o)
* *-property ("no write down"): s should have write access to o only if C(o) >= C(s)
* in practice, subjects are programs (acting on behalf of users)
    * if program accesses secret info, OS ensures that it can't write to confidential file later, even if program does not leak info
    * might need explicit declassification operation for usability purposes

Biba integrity model
* Prevents inappropriate modification of data
* dual of Bell-a Padula
* subjects and objects ordered by integrity classification scheme I(s) and I(o)
* Write access: s can modify o iff I(s) >= I(o)
    * unreliable person cannot modify file containing high integrity info
* Read access: s can only read o only if I(o) >= I(s)
    * unreliable info cannot "contaminate" subject

Low Watermark Property
* Biba's access rules are very restrictive - subject cannot ever read lower integrity object
* dynamic integrity levels:
    * Subject Low Watermark Property: If subject s reads object o, then I(s) = glb(I(s), I(o)) where glb() = greatest lower bound
    * Object Low Watermark Property: If subject s modifies object o, then I(o) = glb(I(s), I(o))
* Integrity of subject/object can only go down, information flows down

Bell-La Padula & Biba are: 
* simple enough to prove properties
* to simple for practical benefit  
    * need declassification
    * need confidentiality and integrity, not just one
    * object creation? 
* info leak possible through covert channels in implementation of model

Information flow control
* info flow policy describes authorized paths along which info can flow
* Bell-La Padula describes lattice-based information flow policy
* in compiler-based info flow control, compiler checks whether info flow can violate an info flow policy
    * Explicit flow: y=x, or y=x/z
    * implicit flow: if x=1, then y=0; else y=1
    * input parameters of program have a associated (lattice-based) security classification
    * compiler goes through program and updates security classification of each variable depending on individual statements that update the variable, suing dynamic BLP/Biba
    * ultimately, security classification for each variable that is output by the program is computed
    * user/another program allowed to see the output if allowed by user's security classification

## Trusted Operating System Design

Trusted OS design elements: 
* Design must address which objects are accessed how and which subjects have access to what
* 8 design principles for security: 
    * least privilege
    * economy of mechanism: simple & straightforward protection mechanism
    * open design: avoid security by obscurity - no secret algorithms
    * complete mediation: every access attempt checked
    * permission based / fail-safe defaults: default should be denial of access
    * separation of privileges: two or more conditions must be met for access
    * least common mechanism: every shared mechanism can be used as covert channel
    * ease of use
* Security features of trusted OS
    * identification & authentication
    * access control
    * object reuse protection
    * complete mediation
    * trusted path
    * accountability and audit
    * intrusion detection

Access control
* Mandatory access control (MAC): central authority establishes who can access what
    * used for military environments
    * for implementing Chinese Wall, Bell-La Padula, Biba
* Discretionary access control (DAC): owners of object have some control over who can access it
    * can grant other access of home directory
    * UNIX and Windows
* RBAC is either MAC nor DAC

Object reuse protection
* OS should erase returned memory before handing it out to other users
* Defensive programming: erase sensitive data yourself before returning to OS   
    * compiler might interfere
* similar problems for files, regsiters, storage media
* Hidden data
    * Examples: deleting file does not physically erase disk, deleting email does not remove it from Google's backups, putting black box over text in PDF

Trusted path
* Gives assurance to users that her keystrokes and mouse clicks are sent to legitimate receiver application
* Quite difficult for Linux and Windows

Accountability and audit
* Keep audit log of all security-related events
* Provides accountability if something goes bad
    * who deleted sensitive records? How did intruder get into system?
* Attacker should not be able to modify log
* Granularity of logs: 
    * Fine-grained logs: space/efficiency problems, hard to find actual attack
    * coarse-grained logs: miss attack, lack of details

Intrusion detection: 
* OS should detect intrusion as it occurs
* usually by correlating actual behaviour with normal behaviour
* alarm if behaviour looks abnormal


Trusted computing base (TCB)
* TCB consiss of the part of a trusted OS that is necessary to enforce OS security policy
    * changing non-TCB part of OS won't affect OS security
    * should be complete and correct
* can be implemented either in different parts of OS, or in a separate security kernel
* security kernel- easier to validate and maintain
    * runs below OS kernel, more difficult for attacker to subvert it

Levels
1. Hardware
2. Security kernel (access control, authentication)
3. Operating system (resource allocation, sharing, hardware interaction)
4. User tasks

Some processors support the layering based on "Rings"
* If processor is operating in ring n, code can access only memory and instructions in rings >= n
* Accesses to rings < n trigger interrupt/exception and inner ring will grant or deny access
* x86 architecture supports 4 rings
* Linux and Windows uses 2 rings: user and supervisor mode (no security kernel)

Reference monitor
* part of TCB
* collection of access controls for devices, files, memory, IPC, ...
* not ncessarily one piece of code
* tamperproof, unbypassable, analyzable
* interacts with other security mechanism

Virtualization
* provide logical separation
* virtual memory: page mapping gives each process the impression of having separate memory space
* virtual machines: virtualize I/O devices, files, printers, ...
    * attacks confined to virtual environment
    * rootkit can make OS run in virtual environment; difficult to detect

Application Insulation
* Memory encryption techniques allow application shielding from other apps
* Application partitioned into trusted and untrusted code
* Trusted code segment encrypted in memory using a key living in secure hardware (close to CPU)
* Untrusted code talks with trusted code via compact API
* Trusted computing base reduced to secure hardware, CPU, and small trusted code

Least privilege in popular OSs
* Windows pre-NT: any user process can do anything
* pre-Vista: fined-grained access control, but many users just run as administrator
* Vista: easier for users to temporarily acquire additional access rights; have integrity levels; eg. IE runs at lowest integrity level, can't overwrite a user's files
* Traditional UNIX: root process has accessto anything, user process has full access to user's data
* SELinux and AppArmor: provides MAC, implement least privilege
    * no root user
    * difficult to setup
* other Unix approaches: Chroot, compartmentalization, SUID

Chroot
* sandbox/jail a command by changing its root directory
    chroot /new/root command
* command cannot run outside of jail
* attackers may break out of jail

Compartmentalization
* split application into parts and apply least privilege to each part
* OpenSSH splits SSH daemon into privleged monitor and unprivileged, jailed child
    * child receives network data from client; may get corrupted
    * child needs to contact monitor to get access to protected info; makes it difficult to corrupt monitor
    * monitor shutdown child if suspicious behaviour

Setuid / suid bit
* UNIX ACLs contain an suid bit
* if suid bit set for executable, the executable will execute under identity of its owner, not identity of caller
    * /usr/bin/passwd belongs to root, has suid bit set
    * if usr calls /usr/bin/passwd, programassume root identity, can thus update password file
* "confused deputy" attack: Eve executes /usr/bin/passwd, convince program it's Alice, can then change Alice's password

Assurance: How to convince others to trust our OS? 
* Testing
    * might be infeasible to test all possible inputs
    * ask outside experts to break into OS
* Formal verfication
    * use mathematical logic to prove correctness of OS
    * OSs growing faster in size than research advances
* Validation: requirements checking, design and code reviews, system testing

Evaluation: trusted entity evlauate OS and certify that OS satisfies some criteria
* well-known sets of criteria: 
    * "Orange Book" (of US Department of Defense): levels of ratings from D to A1; UNIXs are mostly C1
    * The Common Critera (international effort; replace Orange Book)
        * have protection profiles: list security threats and objectives
        * products rated against profiles
        * ratings from EAL 1 to EAL 7